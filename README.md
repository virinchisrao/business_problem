# ğŸ§¾ ecommerce-sales-etl
**Scalable daily-batch ETL for multi-channel sales data (CSV + REST API).**

---

## âœ… What It Does
1. Reads **CSV files** dropped into `data/raw/`  
2. Calls **REST API** (optional) for mobile / web sales  
3. Cleans, validates, and quarantines bad rows  
4. Aggregates:
   - Total sales & revenue per channel  
   - Top 5 best-selling products  
5. Saves:
   - âœ… Clean rows â†’ PostgreSQL table `sales.sales_cleaned`  
   - ğŸ“Š Daily report â†’ `sales.sales_daily_report` (queryable)  
   - ğŸ’¾ CSV snapshot â†’ `output/daily_sales_report_<date>.csv`

---

## ğŸš€ Quick Start (CSV Only)

### 1ï¸âƒ£ Clone the Repo
```bash
git clone https://github.com/<your-username>/ecommerce-sales-etl.git
cd ecommerce-sales-etl
2ï¸âƒ£ Setup PostgreSQL

Create database:

createdb ecommerce_sales
```


Then run the init script:

```bash
psql -d ecommerce_sales -f sql/init.sql
```
3ï¸âƒ£ Configure

Edit config.yaml to add your database credentials.

4ï¸âƒ£ Install Dependencies
```bash
python -m venv venv
```
# Activate virtual environment
# Windows:
```bash
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

pip install -r requirements.txt
```
5ï¸âƒ£ Run the Pipeline

Drop CSV files named like online_sales_2025-11-06.csv into data/raw/ and run:
```bash
python src/main.py
```

### Reports appear in output/; query tables in pgAdmin or psql.

ğŸ”Œ Adding REST API Ingestion
Example Endpoint
GET https://api.example.com/v1/sales?date=2025-11-06
[
  {"product_id":"P101","product_name":"Mouse","quantity":20,"unit_price":25.0,"channel":"mobile"}
]

Add to config.yaml
api:
  url: "https://api.example.com/v1"
  key: "${API_KEY}"   # never hard-code keys

Set API Key

Windows PowerShell
```bash
$env:API_KEY="abc123"

```
Linux/macOS
```bash
export API_KEY=abc123
```

Then rerun:
```bash
python src/main.py

```
If no API is available, comment out or remove the api: section â€” the pipeline automatically falls back to CSV-only mode.

### ğŸ§ª Run Tests
```bash
pip install pytest

# Windows
$env:PYTHONPATH="."; pytest tests -v

# Linux/macOS
PYTHONPATH="." pytest tests -v
```

### âœ… 5 unit tests validate ETL logic.
```bash
ğŸ“ Folder Map
data/raw/          âœ drop CSV files here
data/quarantine/   âœ bad rows (CSV + DB)
output/            âœ daily report CSV
logs/              âœ pipeline_<date>.log
sql/init.sql       âœ creates Postgres schema
src/               âœ ETL modules
tests/             âœ pytest suite
```
### ğŸ” Logs & Error Handling

INFO â†’ Normal progress

ERROR â†’ Logged + printed; pipeline stops (cron retries next day)

Bad rows go to:

Table â†’ sales_quarantine

CSV â†’ data/quarantine/quarantined_<date>.csv

ğŸš¢ Deployment Hints

Schedule daily run â†’ Windows Task Scheduler / cron / Airflow

Containerize â†’ add Dockerfile (example in wiki)

Cloud-ready â†’ point config.yaml to RDS, Supabase, or any Postgres instance

ğŸ¤– AI Usage

70 % boilerplate generated by ChatGPT, including folder tree, SQL, and modules.
Human-refined: JSONB adapter, Windows paths, logging, and tests.
Every AI-generated block is tagged # AI-generated, human-adjusted in code.
Built in Â±2 hours â€” extend freely!
